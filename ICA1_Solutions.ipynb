{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Assignment One\n",
    "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (or HTML of the rendered notebook)  before the end of class.\n",
    "\n",
    "___\n",
    "\n",
    "# Answer Key\n",
    "This is an answer key to the first live-session assignment. Please remember that the assignment is meant to be formative. I want you to be faced with a problem, understand it, struggle with it, and then give your best shot at answering it. This key is meant to be the reflective side of the assignment--where you get to see someone's elegant solution to the problem.\n",
    "\n",
    "Use this key as a learning tool. Perhaps you learn some new python syntax or better understand the setup of the mathematics. Perhaps you really struggled through the assignment and this key can be poured over as a means of learning. \n",
    "\n",
    "You may have cruised through this assignment. Great for you! But not everyone did--so offer help and guidance to those that are struggling to understand python. They have never seen some of the syntax that your brain just interprets as 'readable code', so be conscious of that! \n",
    "\n",
    "Also -- please do not share this notbook with future students for the class! You are robbing them of a formative learning experience where they get to reflect on their own strengths and address weaknesses.\n",
    "________________________________________________________________________________________________________\n",
    "\n",
    "## Loading the Data\n",
    "Please run the following code to read in the \"diabetes\" dataset from sklearn's data loading module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (442, 10)\n",
      "target shape: (442,)\n",
      "range of target: 25.0 346.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "ds = load_diabetes()\n",
    "\n",
    "# this holds the continuous feature data\n",
    "print ('features shape:', ds.data.shape) # there are 442 instances and 10 features per instance\n",
    "print ('target shape:', ds.target.shape)\n",
    "print ('range of target:', np.min(ds.target),np.max(ds.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "##Using Linear Regression \n",
    "In the videos, we derived the optimal values of the regression weights could be found with the following equation (you must be connected to the internet for this equation to show up properly):\n",
    "\n",
    "$$ w = (X^TX)^{-1}X^Ty $$\n",
    "\n",
    "**Question 1:** For the diabetes dataset, how many elements will the vector $w$ contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is: 11 elements are in w\n"
     ]
    }
   ],
   "source": [
    "# The weight vector will have the number of attributes + 1 for the bias term.\n",
    "print ('The answer is:', ds.data.shape[1]+1, 'elements are in w')\n",
    "\n",
    "# The \"shape\" function is a part of numpy matrices. It gives their number of rows and columns\n",
    "# so ds.data.shape is a two element vector [442, 10]. ds.data.shape[1] will return \"10\" as the number of columns\n",
    "# Since there are 10 columns in ds.data, there are 10 features plus the bias term in w\n",
    "# below, we will account for the bias term by adding a vector of ones to ds.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 1:** In the following empty cell, use this equation and numpy matrix operations to find the values of the vector $w$. You will need to be sure $X$ and $y$ are created like the instructor talked about in the video. Don't forget to include any modifications to $X$ to account for the bias term in $w$. You might be interested in the following functions:\n",
    "\n",
    "- `np.hstack((mat1,mat2))` stack two matrices horizontally\n",
    "- `np.ones((rows,cols))`\n",
    "- `my_mat.T` takes transpose of numpy matrix named `my_mat`\n",
    "- `np.dot(mat1,mat2)` is matrix multiplication for two matrices\n",
    "- `np.linalg.inv(mat)` gets the inverse of the variable `mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 152.13348416  -10.01219782 -239.81908937  519.83978679  324.39042769\n",
      " -792.18416163  476.74583782  101.04457032  177.06417623  751.27932109\n",
      "   67.62538639]\n"
     ]
    }
   ],
   "source": [
    "# Write you code here, print the values of w to the console output of the notebook\n",
    "from numpy.linalg import inv # inverse\n",
    "from numpy import dot as mmult # a better name for matrix multiplication\n",
    "\n",
    "# we need to add ones to the vector X so that w will get a bias term\n",
    "# when we solve for w, it will find the weights that best solve the linear regression\n",
    "# problem. Because one feature always has a value of one, an element in w will always be added\n",
    "# to the linear regression. This is the exact same as a bias term.\n",
    "X = np.hstack((np.ones([ds.data.shape[0],1]), ds.data)) \n",
    "#    np.ones([ds.data.shape[0],1]), this code creates a column vector of 442 ones\n",
    "#    np.hstack((~, ds.data)), this code takes the 'ones' vector and stacks it as new column with ds.data\n",
    "\n",
    "y = ds.target # this is what we are trying to predict\n",
    "\n",
    "# now just plug and chug into the formula\n",
    "# I simpified the function using the 'as' keyword at th top of this block\n",
    "#     from numpy.linalg import inv -- this means I can directly call the inv function, instead of saying np.linalg.inv\n",
    "#     from numpy import dot as mmult -- this renames the np.dot function to mmult (a better name IMO)\n",
    "w = inv(mmult(X.T,X)).dot(mmult(X.T,y))\n",
    "print w # the first element in w is the bias term, and the remainging weights multiply the ds.data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 152.13348416]\n",
      " [ -10.01219782]\n",
      " [-239.81908937]\n",
      " [ 519.83978679]\n",
      " [ 324.39042769]\n",
      " [-792.18416163]\n",
      " [ 476.74583782]\n",
      " [ 101.04457032]\n",
      " [ 177.06417623]\n",
      " [ 751.27932109]\n",
      " [  67.62538639]]\n"
     ]
    }
   ],
   "source": [
    "# or you can cast the data as an np.matrix\n",
    "X = np.hstack((np.ones([ds.data.shape[0],1]), ds.data)) \n",
    "Xm = np.matrix(X)\n",
    "ym = np.matrix(y.reshape((len(y),1))) # make it a column vector (numpy issue)!!\n",
    "\n",
    "wm = (Xm.T * Xm)**-1 * Xm.T*ym\n",
    "print wm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 2:** Scikit-learn also has a linear regression fitting implementation. Look at the scikit learn API and learn to use the linear regression method. The API is here: \n",
    "\n",
    "- API Reference: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Use the sklearn `LinearRegression` module to check your results from the previous question. \n",
    "\n",
    "**Question 2**: Did you get the same parameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model coefficients are: [ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "  476.74583782  101.04457032  177.06417623  751.27932109   67.62538639]\n",
      "model intercept is 152.133484163\n",
      "Yes!! they are the same as w, but the bias term is stored separately\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# write your code here, print the values of model by accessing \n",
    "#    it properties that you learned from the API\n",
    "\n",
    "# to use scikit learn, we need to get a linear regression object using this syntax:\n",
    "reg = LinearRegression(fit_intercept=True)\n",
    "# the 'reg' object is a linear model. We can call lots of functions here to train it, predict from it, etc.\n",
    "# think of it as a special class that knows how to map from X to y using linear regression.\n",
    "# this model also takes care of the bias term for us, so we can just give it the original data\n",
    "# there is no need to stack ones onto the matrix--it will do that for us!\n",
    "\n",
    "# Let tell the model what out features and targets look like\n",
    "# Under the hood, thie creates all the weights and bias term using optimization\n",
    "reg.fit(ds.data,ds.target)\n",
    "# after this call, 'reg' is a fitted linear regression model. It has stored all the weights based upon \n",
    "# the training data we gave it\n",
    "\n",
    "print 'model coefficients are:', reg.coef_\n",
    "print 'model intercept is', reg.intercept_\n",
    "print 'Yes!! they are the same as w, but the bias term is stored separately'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "Recall that to predict the output from our model, $\\hat{y}$, from $w$ and $X$ we need to use the following formula:\n",
    "$\\hat{y}=w^TX.T$\n",
    "\n",
    "Where $X$ is a matrix with example instances in each row of the matrix. \n",
    "\n",
    "**Exercise 3:** \n",
    "- *Part A:* Use this matrix multiplication to predict output using numpy and also using the sklearn regression object.\n",
    "- *Part B:* Calculate the mean squared error between your prediction from numpy and the target, $\\sum_i(y-\\hat{y}_{numpy})^2$. \n",
    "- *Part C:* Calculate the mean squared error between your sklearn prediction and the target, $\\sum_i(y-\\hat{y}_{sklearn})^2$.\n",
    " - **Note**: you may need to make the regression weights a column vector using the following code: `w = w.reshape((len(w),1))` This assumes your weights vector is assigned to the variable named `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Sklearn is: 2859.69039877 2859.69039877\n",
      "MSE Numpy is: 2859.69039877 2859.69039877\n"
     ]
    }
   ],
   "source": [
    "# Use this block to answer the questions\n",
    "\n",
    "# we can use numpy to compute the mean squared error of the linear regression model\n",
    "\n",
    "# using the sklearn API is very easy to get what our model predictions are\n",
    "# 'reg' is already trained and has stored the proper linear regression weights\n",
    "# to get predictions from this trained model, we can simply give it features\n",
    "# in this case we are giving it the same features we used for training\n",
    "# but we could be giving it a matrix of features that were not used in training!\n",
    "# as long as the matrix has the same number of columns as the training data, it will try to predict the \n",
    "# outputs of the linear regression.\n",
    "yhat_sklearn = reg.predict(ds.data) # use sklearn to get predicted output\n",
    "\n",
    "\n",
    "# Now, lets use matrix algebra to get the predictions from 'w'\n",
    "w = w.ravel() # you could also use: w.reshape((len(w),1)) to make w a column vector\n",
    "# the ravel function is a special numpy function that basically makes w a column vector \n",
    "# it also fixes a common problem with numpy nesting matrices inside each other \n",
    "# the ravel() ensures that w is shaped properly as a columne vector\n",
    "\n",
    "yhat_numpy = mmult(w.T,X.T) # multiply by the weights, these are the prdictions!\n",
    "# becasue we used ravel, yhat_numpy will also have proper dimensions\n",
    "\n",
    "\n",
    "y = ds.target # assign the target\n",
    "\n",
    "# calculate the mean squared error\n",
    "# this can be done with numpy functions where\n",
    "#   y-yhat returns a vector of the difference in the prediction\n",
    "#   (y-yhat)**2 squares each element, so we can just take the mean\n",
    "MSE_sklearn = np.mean((y-yhat_sklearn)**2)\n",
    "MSE_numpy = np.mean((y-yhat_numpy)**2)\n",
    "\n",
    "# you can also do this with sklearn using built in functions:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_sklearn2 = mean_squared_error(y,yhat_sklearn)\n",
    "MSE_numpy2 = mean_squared_error(y,yhat_numpy)\n",
    "\n",
    "print 'MSE Sklearn is:', MSE_sklearn, MSE_sklearn2\n",
    "print 'MSE Numpy is:', MSE_numpy, MSE_numpy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "## Using Linear Classification\n",
    "Now lets use the code you created to make a classifier with linear boundaries. Run the following code in order to load the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (150L, 4L)\n",
      "original number of classes: 3\n",
      "new number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# this will overwrite the diabetes dataset\n",
    "ds = load_iris() # this overwrites the previous data!! the previous values for ds.data and ds.target are gone from memory now\n",
    "print 'features shape:', ds.data.shape # there are 150 instances and 4 features per instance\n",
    "print 'original number of classes:', len(np.unique(ds.target))\n",
    "\n",
    "# now let's make this a binary classification task\n",
    "ds.target = ds.target>1 \n",
    "print 'new number of classes:', len(np.unique(ds.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 4:** Now use linear regression to come up with a set of weights, `w`, that predict the class value. This is exactly like you did before for the *diabetes* dataset. However, instead of regressing to continuous values, you are just regressing to the integer value of the class (0 or 1), like we talked about in the video. Remember to account for the bias term when constructing the feature matrix, `X`. Print the weights of the linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.68544646 -0.04409841  0.19823256  0.00424001  0.54654271]\n"
     ]
    }
   ],
   "source": [
    "# write your code here and print the values of the weights \n",
    "from numpy.linalg import inv # inverse\n",
    "from numpy import dot as mmult # a better name for matrix multiplication\n",
    "\n",
    "# basically the same code as above. We are getting the regression weights and stacking ones to the feature data\n",
    "X = np.hstack((np.ones([ds.data.shape[0],1]), ds.data))\n",
    "y = ds.target\n",
    "\n",
    "w = inv(mmult(X.T,X)).dot(mmult(X.T,y)) # the weights equation\n",
    "print w.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "**Exercise 5:** Finally, use a hard decision function on the output of the linear regression to make this a binary classifier. This is just like we talked about in the video, where the output of the linear regression passes thrugh a function: \n",
    "\n",
    "$\\hat{y}=g(w^TX.T)$ where $g(w^TX.T) < \\alpha$ means the predicted class is `0` and $g(w^TX.T) >= \\alpha$ means the predicted class is `1`. \n",
    "\n",
    "Here, alpha is a threshold for deciding the class. \n",
    "\n",
    "**Question 3**: What value for $\\alpha$ makes the most sense? What is the accuracy of the classifier given the $\\alpha$ you chose? \n",
    "\n",
    "Note: You can calculate the accuracy with the following code: `accuracy = float(sum(yhat==y)) / len(y)` assuming you choose variable names `y` and `yhat` for the target and prediction, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage accuracy: 92.6666666667\n",
      "Percentage accuracy from looping through alphas: 94.6666666667\n",
      "Percentage accuracy from calculating stats: 94.0\n"
     ]
    }
   ],
   "source": [
    "# use this box to predict the classification output\n",
    "yhat = mmult(w.T,X.T) # multiply by the weights\n",
    "alpha = 0.5 # directly between the two class values\n",
    "yhat = yhat > alpha # hard limit decision function\n",
    "\n",
    "accuracy = float(sum(yhat==y)) / len(y)  \n",
    "print 'Percentage accuracy:', accuracy*100\n",
    "# this answer is perfectly fine!\n",
    "\n",
    "#===================Supplemental Exploration=======================\n",
    "# Let's try something a bit more complicated to get the thoughts process going:\n",
    "# maybe we want to try something interesting on the data\n",
    "# we could also try to fine tune the alpha value so that it gets a more accurate representation\n",
    "yhat = mmult(w.T,X.T) # multiply by the weights\n",
    "\n",
    "best_alpha = 0\n",
    "best_acc = 0\n",
    "for thresh in np.linspace(0,1,num=50):\n",
    "    yhat_temp = yhat > thresh\n",
    "    acc_temp = float(sum(yhat_temp==y)) / len(y)\n",
    "    if acc_temp > best_acc:\n",
    "        best_acc = acc_temp\n",
    "        best_alpha = thresh\n",
    "\n",
    "yhat = yhat > best_alpha # hard limit decision function\n",
    "\n",
    "accuracy = float(sum(yhat==y)) / len(y) \n",
    "print 'Percentage accuracy from looping through alphas:', accuracy*100\n",
    "\n",
    "# ========Is there an even better way to get close to optimal without a loop?=========\n",
    "# we already have the ground truth, can we use that\n",
    "\n",
    "yhat = mmult(w.T,X.T) # multiply by the weights\n",
    "\n",
    "mu1 = float(yhat[np.where(y==0)].mean())\n",
    "v1  = yhat[np.where(y==0)].var()\n",
    "\n",
    "mu2 = yhat[np.where(y==1)].mean()\n",
    "v2  = yhat[np.where(y==1)].var()\n",
    "\n",
    "# split the difference between them:\n",
    "alpha3 = mu1 + (mu2-mu1)*float(v1)/(v1+v2) \n",
    "# why would I do something like this??? Can you guess why??\n",
    "\n",
    "yhat = yhat > alpha3 # hard limit decision function\n",
    "\n",
    "accuracy = float(sum(yhat==y)) / len(y) \n",
    "print 'Percentage accuracy from calculating stats:', accuracy*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "That's all! Please **upload your rendered notebook** and please include **team member names** in the notebook submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
